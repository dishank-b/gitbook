# Tokenziation

Tokenization is a way to break down a string of continuous text into small individual chunks called tokens, each token can be a word/character/sub-word.&#x20;

Different tokenization schemes can be used based on different aspects.&#x20;

{% embed url="https://huggingface.co/learn/nlp-course/chapter2/4" %}

