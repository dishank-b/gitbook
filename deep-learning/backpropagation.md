# Automatic Differentiation

Frameworks such as tensorflow or pytorch use auto-differentiation to find the gradient for back propagation of gradients.

Two types of auto-differentiation:

* Forward Mode
* Reverse Mode

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

{% embed url="https://theoryandpractice.org/stats-ds-book/autodiff-tutorial.html#recap" %}

{% embed url="https://auto-ed.readthedocs.io/en/latest/mod3.html" %}

### Concise Comparison of Forward vs Backward Model

{% embed url="https://liqimai.github.io/blog/Forward-Automatic-Differentiation/" %}

{% embed url="https://www.cs.ubc.ca/~fwood/CS340/lectures/AD1.pdf" %}

## vJp and backward pass

{% embed url="https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf" %}

## Hessians in AD

{% embed url="https://iclr-blogposts.github.io/2024/blog/bench-hvp/" %}

### Intuitive way of learning why the hessian vector product is all that we need?&#x20;



{% embed url="https://chatgpt.com/share/67e09a6a-75c4-8007-a03d-f84b2b049815" %}
